{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7665eb5c-c6f1-443d-a5c3-a9568d079c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Creo la SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EsempioDataFrame\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5c728109-159c-4166-ae07-e95d843356e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista di tuple (CF, Nome, Salario)\n",
    "dati = [\n",
    "    (\"RSSMRA80A01H501U\", \"Mario Rossi\", 2500),\n",
    "    (\"VRDLGI85B12H501T\", \"Giulia Verdi\", 3200),\n",
    "    (\"BNCLCU90C23H501Q\", \"Lucia Bianchi\", 2800),\n",
    "    (\"FRCNNA92D10H501P\", \"Anna Franco\", 4000)\n",
    "]\n",
    "\n",
    "# Definisco colonne\n",
    "colonne = [\"CF\", \"NOME\", \"SALARIO\"]\n",
    "\n",
    "# Creo DataFrame\n",
    "df = spark.createDataFrame(dati, schema=colonne)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a1205494-c2e3-4c2a-87d4-e05267e76c0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2560.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 382.0 failed 1 times, most recent failure: Lost task 0.0 in stage 382.0 (TID 265) (DESKTOP-CGOB02T executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 33 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 33 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4964/3441711306.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Mostro i dati\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     def _show_string(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    314\u001b[0m                 )\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1362\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    328\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2560.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 382.0 failed 1 times, most recent failure: Lost task 0.0 in stage 382.0 (TID 265) (DESKTOP-CGOB02T executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 33 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 33 more\r\n"
     ]
    }
   ],
   "source": [
    "# Mostro i dati\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17b66ae-6001-4817-ac5e-cb44d8888324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3eb90e21-586d-464c-85ef-950580dd5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType, IntegerType\n",
    "from pyspark.sql.functions import current_date, col, to_date, when, current_timestamp, monotonically_increasing_id, lit, upper, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "954173b4-8fb2-44e3-84c8-6bce370a8baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestoreFlussoDipendenti:\n",
    "    def __init__(self, path_log_gestore):\n",
    "        self.contatore_idrun = 0\n",
    "        self.path_log_gestore = path_log_gestore\n",
    "\n",
    "        # Logger “superiore”\n",
    "        self.logger = logging.getLogger(\"gestore\")\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        # Handler file, modalità 'w' = sovrascrive il log ogni volta\n",
    "        file_handler = logging.FileHandler(self.path_log_gestore, mode='w')\n",
    "        formatter = logging.Formatter(\"%(message)s\")\n",
    "        file_handler.setFormatter(formatter)\n",
    "\n",
    "        # Evita di aggiungere più handler se ricrei oggetti\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(file_handler)\n",
    "\n",
    "    def genera_idrun(self):\n",
    "        self.contatore_idrun += 1\n",
    "        return self.contatore_idrun\n",
    "\n",
    "    def processa_flusso(self, path_csv, path_log):\n",
    "        idrun = self.genera_idrun()\n",
    "        \n",
    "        # Creo oggetto TabellaDipendenti\n",
    "        tabella = TabellaDipendenti(path_csv, path_log, idrun)\n",
    "        \n",
    "        # Recupero il dataframe grezzo\n",
    "        df_grezzo = tabella.GivemeDataFrame()\n",
    "        \n",
    "        # Applico pulizia automatica\n",
    "        tab_ok, tab_scarti = tabella.pulisci()\n",
    "        \n",
    "        self.logger.info(f\"IDRUN={idrun} - Tabelle create: df_grezzo={df_grezzo.count()}, tab_ok={tab_ok.df.count()}, tab_scarti={tab_scarti.df.count()}, Data={datetime.now()}\"\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Restituisco tutto\n",
    "        return df_grezzo, tab_ok, tab_scarti\n",
    "    \n",
    "    def filtra_salario(self, tabella, soglia):\n",
    "        \"\"\"\n",
    "        Restituisce un DataFrame filtrato per SALARIO > soglia\n",
    "        tabella: oggetto TabellaDipendenti (ok o scarti)\n",
    "        \"\"\"\n",
    "        df = tabella.GivemeDataFrame()\n",
    "        self.logger.info(f\"{datetime.now()} - IDRUN={tabella.idrun} - Operazione: filtra_salario > {soglia}, Righe risultanti: {df.count()}\")\n",
    "        return df.select(\"IDRUN\",\"CF\",\"NOME\",\"SALARIO\").where(col(\"SALARIO\") >= soglia)\n",
    "    \n",
    "    def seleziona_colonne(self, tabella, colonne):\n",
    "        \"\"\"\n",
    "        Restituisce un DataFrame con solo le colonne richieste\n",
    "        \"\"\"\n",
    "        df = tabella.GivemeDataFrame()\n",
    "        self.logger.info(\n",
    "        f\"{datetime.now()} - IDRUN={tabella.idrun} - Operazione: seleziona_colonne, Colonne={colonne}, Righe risultanti={df.count()}\"\n",
    "        )\n",
    "        return df.select(*colonne)\n",
    "    \n",
    "    def filtra_nomi(self, tabella, pattern):\n",
    "        \"\"\"\n",
    "        Restituisce tutti i record in cui il nome contiene la stringa `pattern`, \n",
    "        ignorando maiuscole/minuscole.\n",
    "\n",
    "        tabella: oggetto TabellaDipendenti (ok o scarti)\n",
    "        pattern: stringa da cercare\n",
    "        \"\"\"\n",
    "        df = tabella.GivemeDataFrame()\n",
    "        df_lower = df.withColumn(\"NOME_lower\", lower(col(\"NOME\")))\n",
    "\n",
    "        pattern = pattern.lower()  # tutto minuscolo\n",
    "\n",
    "        df_filtrato = df_lower.filter(col(\"NOME_lower\").rlike(pattern))\n",
    "        \n",
    "        self.logger.info(\n",
    "        f\"{datetime.now()} - IDRUN={tabella.idrun} - Operazione: filtra_nomi, Pattern='{pattern}', Righe risultanti={df_filtrato.count()}\"\n",
    "        )\n",
    "\n",
    "        # ritorna solo le colonne principali\n",
    "        return df_filtrato.select(\"IDRUN\",\"CF\",\"NOME\",\"SALARIO\",\"DINS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dce0de13-c2cf-41d2-b080-ba2d90027c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabellaDipendenti:\n",
    "\n",
    "    \n",
    "    # Costruttore\n",
    "    def __init__(self, path_csv, path_log, idrun):\n",
    "        self.spark = SparkSession.builder.master(\"local[1]\").appName(\"FlussoDipendenti\").getOrCreate()\n",
    "        schema = StructType([\n",
    "            StructField(\"CF\", StringType(), True),\n",
    "            StructField(\"NOME\", StringType(), True),\n",
    "            StructField(\"DN\", StringType(), True),\n",
    "            StructField(\"SALARIO\", StringType(), True)\n",
    "        ])\n",
    "        self.idrun = idrun\n",
    "        self.path_log = path_log\n",
    "        \n",
    "        # leggo i dati dal flusso\n",
    "        self.df = self.spark.read.csv(path_csv, header=True, schema=schema)\n",
    "        self.df = self.df.withColumn(\"DINS\", current_date())  \n",
    "        \n",
    "        logging.basicConfig(\n",
    "        filename=self.path_log,\n",
    "        filemode='w',\n",
    "        level=logging.INFO,\n",
    "        format=\"%(message)s\",\n",
    "        force=True\n",
    "        )\n",
    "        \n",
    "        logging.info(\"IDRUN=%s, Operazione=Costruttore, Stato=OK, File=%s, Data=%s\", \n",
    "             idrun, path_csv, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    # Funzione che mostra la \"tabella\". \n",
    "    def show(self):\n",
    "        if self.df:\n",
    "            self.df.show(truncate=False)\n",
    "            logging.info(\"IDRUN=%s, Operazione=Show, Stato=OK, Righe=%s, Data=%s\", \n",
    "             self.idrun, self.df.count(), datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        else:\n",
    "            print(\"DataFrame vuoto.\")\n",
    "            logging.info(\"IDRUN=%s, Operazione=Show, Stato=VUOTO, Data=%s\", \n",
    "             self.idrun, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    def GivemeDataFrame(self):\n",
    "        logging.info(\"IDRUN=%s, Operazione=GivemeDataFrame, Stato=OK, Data=%s\", \n",
    "             self.idrun, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        return self.df\n",
    "    \n",
    "    def pulisci(self):\n",
    "        \"\"\"\n",
    "        Filtra i dati: ritorna due oggetti TabellaDipendenti,\n",
    "        uno con i dati validi (OK) e uno con i dati scartati (KO).\n",
    "        La tabella OK ha colonne convertite, la tabella scarti rimane stringa.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        # 1. SALARIO numerico e positivo\n",
    "        df_valid_salario = df.withColumn(\n",
    "            \"valid_salario\",\n",
    "            when(col(\"SALARIO\").rlike(\"^[0-9]+(\\.[0-9]+)?$\") & (col(\"SALARIO\").cast(\"double\") > 0), True).otherwise(False)\n",
    "        )\n",
    "        logging.info(\"IDRUN=%s, Operazione=pulisci, Filtro=SALARIO, Righe=%s, Data=%s\",\n",
    "                 self.idrun, df_valid_salario.filter(col(\"valid_salario\")==True).count(),\n",
    "                 datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "        # 2. DN valido formato YYYY-MM-DD e giorno/mese coerenti\n",
    "        df_valid_date = df_valid_salario.withColumn(\n",
    "            \"valid_dn\",\n",
    "            when(\n",
    "                (col(\"DN\").rlike(r'^\\d{4}-\\d{2}-\\d{2}$')) &\n",
    "                (col(\"DN\").substr(6,2).cast(\"int\").between(1,12)) &\n",
    "                (col(\"DN\").substr(9,2).cast(\"int\").between(1,31)),\n",
    "                True\n",
    "            ).otherwise(False)\n",
    "        )\n",
    "        logging.info(\"IDRUN=%s, Operazione=pulisci, Filtro=DN, Righe=%s, Data=%s\",\n",
    "                 self.idrun, df_valid_date.filter(col(\"valid_dn\")==True).count(),\n",
    "                 datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "        # 3. NOME senza numeri e non nullo\n",
    "        df_valid_name = df_valid_date.withColumn(\n",
    "            \"valid_nome\",\n",
    "            when(col(\"NOME\").isNotNull() & (~col(\"NOME\").rlike(r\"[0-9]\")), True).otherwise(False)\n",
    "        )\n",
    "        logging.info(\"IDRUN=%s, Operazione=pulisci, Filtro=NOME, Righe=%s, Data=%s\",\n",
    "                 self.idrun, df_valid_name.filter(col(\"valid_nome\")==True).count(),\n",
    "                 datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "        # 4. CF lungo 16 e non nullo\n",
    "        df_valid_cf = df_valid_name.withColumn(\n",
    "            \"valid_cf\",\n",
    "            when(col(\"CF\").isNotNull() & col(\"CF\").rlike(\"^.{16}$\"), True).otherwise(False)\n",
    "        )\n",
    "        logging.info(\"IDRUN=%s, Operazione=pulisci, Filtro=CF, Righe=%s, Data=%s\",\n",
    "                 self.idrun, df_valid_cf.filter(col(\"valid_cf\")==True).count(),\n",
    "                 datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "        # 5. Colonna \"valid\" finale\n",
    "        df_valid_cf = df_valid_cf.withColumn(\n",
    "            \"valid\",\n",
    "            col(\"valid_salario\") & col(\"valid_dn\") & col(\"valid_nome\") & col(\"valid_cf\")\n",
    "        )\n",
    "        logging.info(\"IDRUN=%s, Operazione=pulisci, Totale righe OK=%s, Totale righe KO=%s, Data=%s\",\n",
    "                 self.idrun, df_valid_cf.filter(col(\"valid\")==True).count(),\n",
    "                 df_valid_cf.filter(col(\"valid\")==False).count(),\n",
    "                 datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "        # 6. DF OK → righe valide\n",
    "        df_ok = df_valid_cf.filter(col(\"valid\") == True) \\\n",
    "            .withColumn(\"IDRUN\", lit(self.idrun)) \\\n",
    "            .withColumn(\"SALARIO\", col(\"SALARIO\").cast(\"double\")) \\\n",
    "            .withColumn(\"DN\", to_date(\"DN\", \"yyyy-MM-dd\")) \\\n",
    "            .withColumn(\"DINS\", current_date()) \\\n",
    "            .drop(\"valid_salario\", \"valid_dn\", \"valid_nome\", \"valid_cf\", \"valid\") \\\n",
    "            .select(\"IDRUN\", \"CF\", \"NOME\", \"DN\", \"SALARIO\", \"DINS\")\n",
    "\n",
    "        # 7. DF scarti → righe non valide, tutte stringhe, nessuna conversione\n",
    "        df_scarti = df_valid_cf.filter(col(\"valid\") == False) \\\n",
    "            .withColumn(\"IDRUN\", lit(self.idrun)) \\\n",
    "            .withColumn(\"DINS\", current_date()) \\\n",
    "            .drop(\"valid_salario\", \"valid_dn\", \"valid_nome\", \"valid_cf\", \"valid\") \\\n",
    "            .select(\"IDRUN\", \"CF\", \"NOME\", \"DN\", \"SALARIO\", \"DINS\")\n",
    "        \n",
    "        logging.info(\"IDRUN=%s, Operazione=pulisci, DF_OK righe=%s, DF_SCARTI righe=%s, Data=%s\",\n",
    "                 self.idrun, df_ok.count(), df_scarti.count(),\n",
    "                 datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "        # 8. Creo nuovi oggetti TabellaDipendenti senza leggere CSV\n",
    "        tabella_ok = TabellaDipendenti.__new__(TabellaDipendenti)\n",
    "        tabella_ok.spark = self.spark\n",
    "        tabella_ok.df = df_ok\n",
    "        tabella_ok.idrun = self.idrun\n",
    "\n",
    "        tabella_scarti = TabellaDipendenti.__new__(TabellaDipendenti)\n",
    "        tabella_scarti.spark = self.spark\n",
    "        tabella_scarti.df = df_scarti\n",
    "        tabella_scarti.idrun = self.idrun\n",
    "\n",
    "        return tabella_ok, tabella_scarti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "81d8b778-19b2-41be-8626-af88a4d98f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_log_gestore = \"../logs/tlog_gestore.txt\"\n",
    "gestore = GestoreFlussoDipendenti(path_log_gestore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "45e6f39c-ff50-49db-bfde-27693279b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_log = \"../logs/tlog.txt\"\n",
    "path_flusso = \"../data/Flusso.csv\"\n",
    "\n",
    "df_grezzo, tab_ok, tab_scarti = gestore.processa_flusso(path_flusso, path_log)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c743981-aafb-42e8-ae5b-609f6ff1f35c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1935928b-630c-4f60-bc58-7057d493d4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+-----------------+-----------+-------+----------+\n",
      "|IDRUN|CF               |NOME             |DN         |SALARIO|DINS      |\n",
      "+-----+-----------------+-----------------+-----------+-------+----------+\n",
      "|1    |RSSMRA85M01H501Z |Mario Rossi      |1985-03-15 |-2500  |2025-09-26|\n",
      "|1    |VRDLCN80C15D612Y |Claudia Verdi    |15-08-1980 |2800   |2025-09-26|\n",
      "|1    |DMRCNZ92E18F205U |Daniele Moroni   |1992-06-188|3000   |2025-09-26|\n",
      "|1    |SPRTMN7820G345T  |Simone Sportiello|1978-20-20 |-500   |2025-09-26|\n",
      "|1    |CNCLNZ87C30D612R |Concetta Lanza   |30-06-1987 |fff    |2025-09-26|\n",
      "|1    |LDDRCR99R14F205U |Riccardo Loddo   |1999-14-04 |2000   |2025-09-26|\n",
      "|1    |LDDRCR99R14F205U |Riccardo Pippo   |1999-04-14 |abc    |2025-09-26|\n",
      "|1    |NGRMRT75D10E345W |Ma3t1na Polgatti |1975-10-10 |3100   |2025-09-26|\n",
      "|1    |NGRMRT75345W     |Maria Santa1     |1975-??-10 |3100   |2025-09-26|\n",
      "|1    |NGRMRT75345W     |Maria Santa2     |1975-10-10 |-100   |2025-09-26|\n",
      "|1    |NGRMRT75345W     |Maria Santa3     |1975-10-10 |200    |2025-09-26|\n",
      "|1    |ABCD1234EF567890 |Z0l0 Rossi       |1980-13-01 |1500   |2025-09-26|\n",
      "|1    |LMNOPQ12RST345678|Giulia1 Bianchi  |1990-02-30 |2800   |2025-09-26|\n",
      "|1    |QRSTUV98XYZ123456|Alessandro Verdi |1995-11-31 |abc    |2025-09-26|\n",
      "|1    |WXYZ1234ABCD56789|Federica Rossi   |1992-00-12 |0      |2025-09-26|\n",
      "+-----+-----------------+-----------------+-----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tab_scarti.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0f4b0236-e4ac-4a50-978f-43416cf93058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+--------------+-------+\n",
      "|IDRUN|CF              |NOME          |SALARIO|\n",
      "+-----+----------------+--------------+-------+\n",
      "|1    |BNCLGU90B22F205X|Luca Bianchi  |3200.0 |\n",
      "|1    |NGRMRT75D10E345W|Martina Neri  |3100.0 |\n",
      "|1    |PPLGRD91D05E345Q|Paolo Pugliese|3400.0 |\n",
      "|1    |NGRMRT75D10E345W|Maria Santa   |3100.0 |\n",
      "+-----+----------------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtrato = gestore.filtra_salario(tab_ok, 3000)\n",
    "df_filtrato.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "760f26c4-8b43-4ff0-ad43-daf23ea5d69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+-------+\n",
      "|IDRUN|              CF|SALARIO|\n",
      "+-----+----------------+-------+\n",
      "|    1|RSSMRA85M01H501Z| 2500.0|\n",
      "|    1|BNCLGU90B22F205X| 3200.0|\n",
      "|    1|NGRMRT75D10E345W| 3100.0|\n",
      "|    1|FBLGPP88A12H501V| 2700.0|\n",
      "|    1|GRLFRN83B25H501S| 2900.0|\n",
      "|    1|PPLGRD91D05E345Q| 3400.0|\n",
      "|    1|NGRMRT75D10E345W| 3100.0|\n",
      "+-----+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selezionato = gestore.seleziona_colonne(tab_ok, [\"IDRUN\",\"CF\",\"SALARIO\"])\n",
    "df_selezionato.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "47e80e3b-a459-4c8e-894f-83c7a410c729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+------------+-------+----------+\n",
      "|IDRUN|CF              |NOME        |SALARIO|DINS      |\n",
      "+-----+----------------+------------+-------+----------+\n",
      "|1    |NGRMRT75D10E345W|Martina Neri|3100.0 |2025-09-26|\n",
      "+-----+----------------+------------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tutti i nomi che contengono \"ina\" (inizio, fine o interno)\n",
    "df_ina = gestore.filtra_nomi(tab_ok, \"ina\")\n",
    "df_ina.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b9f3550-53f3-42d0-a28f-11f4c9714f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------+-------+----------+\n",
      "|CF               |NOME             |DN         |SALARIO|DINS      |\n",
      "+-----------------+-----------------+-----------+-------+----------+\n",
      "|RSSMRA85M01H501Z |Mario Rossi      |1985-03-15 |2500   |2025-09-26|\n",
      "|RSSMRA85M01H501Z |Mario Rossi      |1985-03-15 |-2500  |2025-09-26|\n",
      "|BNCLGU90B22F205X |Luca Bianchi     |1990-07-22 |3200   |2025-09-26|\n",
      "|VRDLCN80C15D612Y |Claudia Verdi    |15-08-1980 |2800   |2025-09-26|\n",
      "|NGRMRT75D10E345W |Martina Neri     |1975-12-10 |3100   |2025-09-26|\n",
      "|FBLGPP88A12H501V |Filippo Baldi    |1988-01-12 |2700   |2025-09-26|\n",
      "|DMRCNZ92E18F205U |Daniele Moroni   |1992-06-188|3000   |2025-09-26|\n",
      "|SPRTMN7820G345T  |Simone Sportiello|1978-20-20 |-500   |2025-09-26|\n",
      "|GRLFRN83B25H501S |Giorgia Ferri    |1983-02-25 |2900   |2025-09-26|\n",
      "|CNCLNZ87C30D612R |Concetta Lanza   |30-06-1987 |fff    |2025-09-26|\n",
      "|PPLGRD91D05E345Q |Paolo Pugliese   |1991-05-05 |3400   |2025-09-26|\n",
      "|LDDRCR99R14F205U |Riccardo Loddo   |1999-14-04 |2000   |2025-09-26|\n",
      "|LDDRCR99R14F205U |Riccardo Pippo   |1999-04-14 |abc    |2025-09-26|\n",
      "|NGRMRT75D10E345W |Ma3t1na Polgatti |1975-10-10 |3100   |2025-09-26|\n",
      "|NGRMRT75D10E345W |Maria Santa      |1975-10-10 |3100   |2025-09-26|\n",
      "|NGRMRT75345W     |Maria Santa1     |1975-??-10 |3100   |2025-09-26|\n",
      "|NGRMRT75345W     |Maria Santa2     |1975-10-10 |-100   |2025-09-26|\n",
      "|NGRMRT75345W     |Maria Santa3     |1975-10-10 |200    |2025-09-26|\n",
      "|ABCD1234EF567890 |Z0l0 Rossi       |1980-13-01 |1500   |2025-09-26|\n",
      "|LMNOPQ12RST345678|Giulia1 Bianchi  |1990-02-30 |2800   |2025-09-26|\n",
      "+-----------------+-----------------+-----------+-------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "path_log = \"../logs/tlog.txt\"\n",
    "path_flusso = \"../data/Flusso.csv\"\n",
    "\n",
    "tabella1 = TabellaDipendenti(path_flusso, path_log, 1)\n",
    "tabella1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532f62ee-3d76-48a4-aacf-57b703981fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------+-------+----------+\n",
      "|CF               |NOME             |DN         |SALARIO|DINS      |\n",
      "+-----------------+-----------------+-----------+-------+----------+\n",
      "|TRNDMN85B12C123X |Andrea Trenti    |1985-02-12 |2600   |2025-09-26|\n",
      "|TRNDMN85B12C123X |Andrea Trenti    |1985-02-12 |-2600  |2025-09-26|\n",
      "|PVRGLU90F22D456Y |Laura Pavarini   |1990-07-22 |3300   |2025-09-26|\n",
      "|GMLRNC80D31E789Z |Claudio Gemelli  |31-09-1980 |2900   |2025-09-26|\n",
      "|MRCDSP75E10F234W |Simona Mercadante|1975-11-10 |3150   |2025-09-26|\n",
      "|FNCGLP88G15H501V |Filippo Fanciulli|1988-01-15 |2750   |2025-09-26|\n",
      "|DMRCNZ92E18F205U |Daniele Moroni   |1992-06-188|3050   |2025-09-26|\n",
      "|SPRTMN78A20G345T |Simone Sportiello|1978-20-20 |-550   |2025-09-26|\n",
      "|GRLFRN83B25H501S |Giorgia Ferri    |1983-02-25 |2950   |2025-09-26|\n",
      "|CNCLNZ87C30D612R |Concetta Lanza   |30-06-1987 |xxx    |2025-09-26|\n",
      "|PPLGRD91D05E345Q |Paolo Pugliese   |1991-05-05 |3450   |2025-09-26|\n",
      "|LDDRCR99R14F205U |Riccardo Loddo   |1999-14-04 |2100   |2025-09-26|\n",
      "|LDDRCR99R14F205U |Riccardo Pippo   |1999-04-14 |abc    |2025-09-26|\n",
      "|NGRMRT75D10E345W |Martina Polgatti |1975-10-10 |3100   |2025-09-26|\n",
      "|NGRMRT75D10E345W |Maria Santa      |1975-10-10 |3100   |2025-09-26|\n",
      "|NGRMRT75345W     |Maria Santa1     |1975-??-10 |3100   |2025-09-26|\n",
      "|NGRMRT75345W     |Maria Santa2     |1975-10-10 |-150   |2025-09-26|\n",
      "|NGRMRT75345W     |Maria Santa3     |1975-10-10 |250    |2025-09-26|\n",
      "|ABCD1234EF567890 |Z0l0 Rossi       |1980-13-01 |1600   |2025-09-26|\n",
      "|LMNOPQ12RST345678|Giulia1 Bianchi  |1990-02-30 |2850   |2025-09-26|\n",
      "+-----------------+-----------------+-----------+-------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "tabella2 = TabellaDipendenti(\"../data/Flusso2.csv\", 2)\n",
    "tabella2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7312965d-ec43-4797-851c-d1732838b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabella1_ok, tabella1_scarti = tabella1.pulisci()\n",
    "tabella2_ok, tabella2_scarti = tabella2.pulisci()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90cc61d5-36f1-420c-80be-f456d5379b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tabella1 OK ===\n",
      "+-----+----------------+--------------+----------+-------+----------+\n",
      "|IDRUN|CF              |NOME          |DN        |SALARIO|DINS      |\n",
      "+-----+----------------+--------------+----------+-------+----------+\n",
      "|1    |RSSMRA85M01H501Z|Mario Rossi   |1985-03-15|2500.0 |2025-09-26|\n",
      "|1    |BNCLGU90B22F205X|Luca Bianchi  |1990-07-22|3200.0 |2025-09-26|\n",
      "|1    |NGRMRT75D10E345W|Martina Neri  |1975-12-10|3100.0 |2025-09-26|\n",
      "|1    |FBLGPP88A12H501V|Filippo Baldi |1988-01-12|2700.0 |2025-09-26|\n",
      "|1    |GRLFRN83B25H501S|Giorgia Ferri |1983-02-25|2900.0 |2025-09-26|\n",
      "|1    |PPLGRD91D05E345Q|Paolo Pugliese|1991-05-05|3400.0 |2025-09-26|\n",
      "|1    |NGRMRT75D10E345W|Maria Santa   |1975-10-10|3100.0 |2025-09-26|\n",
      "+-----+----------------+--------------+----------+-------+----------+\n",
      "\n",
      "=== Tabella1 Scarti ===\n",
      "+-----+-----------------+-----------------+-----------+-------+----------+\n",
      "|IDRUN|CF               |NOME             |DN         |SALARIO|DINS      |\n",
      "+-----+-----------------+-----------------+-----------+-------+----------+\n",
      "|1    |RSSMRA85M01H501Z |Mario Rossi      |1985-03-15 |-2500  |2025-09-26|\n",
      "|1    |VRDLCN80C15D612Y |Claudia Verdi    |15-08-1980 |2800   |2025-09-26|\n",
      "|1    |DMRCNZ92E18F205U |Daniele Moroni   |1992-06-188|3000   |2025-09-26|\n",
      "|1    |SPRTMN7820G345T  |Simone Sportiello|1978-20-20 |-500   |2025-09-26|\n",
      "|1    |CNCLNZ87C30D612R |Concetta Lanza   |30-06-1987 |fff    |2025-09-26|\n",
      "|1    |LDDRCR99R14F205U |Riccardo Loddo   |1999-14-04 |2000   |2025-09-26|\n",
      "|1    |LDDRCR99R14F205U |Riccardo Pippo   |1999-04-14 |abc    |2025-09-26|\n",
      "|1    |NGRMRT75D10E345W |Ma3t1na Polgatti |1975-10-10 |3100   |2025-09-26|\n",
      "|1    |NGRMRT75345W     |Maria Santa1     |1975-??-10 |3100   |2025-09-26|\n",
      "|1    |NGRMRT75345W     |Maria Santa2     |1975-10-10 |-100   |2025-09-26|\n",
      "|1    |NGRMRT75345W     |Maria Santa3     |1975-10-10 |200    |2025-09-26|\n",
      "|1    |ABCD1234EF567890 |Z0l0 Rossi       |1980-13-01 |1500   |2025-09-26|\n",
      "|1    |LMNOPQ12RST345678|Giulia1 Bianchi  |1990-02-30 |2800   |2025-09-26|\n",
      "|1    |QRSTUV98XYZ123456|Alessandro Verdi |1995-11-31 |abc    |2025-09-26|\n",
      "|1    |WXYZ1234ABCD56789|Federica Rossi   |1992-00-12 |0      |2025-09-26|\n",
      "+-----+-----------------+-----------------+-----------+-------+----------+\n",
      "\n",
      "=== Tabella1 OK ===\n",
      "+-----+----------------+-----------------+----------+-------+----------+\n",
      "|IDRUN|CF              |NOME             |DN        |SALARIO|DINS      |\n",
      "+-----+----------------+-----------------+----------+-------+----------+\n",
      "|2    |TRNDMN85B12C123X|Andrea Trenti    |1985-02-12|2600.0 |2025-09-26|\n",
      "|2    |PVRGLU90F22D456Y|Laura Pavarini   |1990-07-22|3300.0 |2025-09-26|\n",
      "|2    |MRCDSP75E10F234W|Simona Mercadante|1975-11-10|3150.0 |2025-09-26|\n",
      "|2    |FNCGLP88G15H501V|Filippo Fanciulli|1988-01-15|2750.0 |2025-09-26|\n",
      "|2    |GRLFRN83B25H501S|Giorgia Ferri    |1983-02-25|2950.0 |2025-09-26|\n",
      "|2    |PPLGRD91D05E345Q|Paolo Pugliese   |1991-05-05|3450.0 |2025-09-26|\n",
      "|2    |NGRMRT75D10E345W|Martina Polgatti |1975-10-10|3100.0 |2025-09-26|\n",
      "|2    |NGRMRT75D10E345W|Maria Santa      |1975-10-10|3100.0 |2025-09-26|\n",
      "+-----+----------------+-----------------+----------+-------+----------+\n",
      "\n",
      "=== Tabella1 Scarti ===\n",
      "+-----+-----------------+-----------------+-----------+-------+----------+\n",
      "|IDRUN|CF               |NOME             |DN         |SALARIO|DINS      |\n",
      "+-----+-----------------+-----------------+-----------+-------+----------+\n",
      "|2    |TRNDMN85B12C123X |Andrea Trenti    |1985-02-12 |-2600  |2025-09-26|\n",
      "|2    |GMLRNC80D31E789Z |Claudio Gemelli  |31-09-1980 |2900   |2025-09-26|\n",
      "|2    |DMRCNZ92E18F205U |Daniele Moroni   |1992-06-188|3050   |2025-09-26|\n",
      "|2    |SPRTMN78A20G345T |Simone Sportiello|1978-20-20 |-550   |2025-09-26|\n",
      "|2    |CNCLNZ87C30D612R |Concetta Lanza   |30-06-1987 |xxx    |2025-09-26|\n",
      "|2    |LDDRCR99R14F205U |Riccardo Loddo   |1999-14-04 |2100   |2025-09-26|\n",
      "|2    |LDDRCR99R14F205U |Riccardo Pippo   |1999-04-14 |abc    |2025-09-26|\n",
      "|2    |NGRMRT75345W     |Maria Santa1     |1975-??-10 |3100   |2025-09-26|\n",
      "|2    |NGRMRT75345W     |Maria Santa2     |1975-10-10 |-150   |2025-09-26|\n",
      "|2    |NGRMRT75345W     |Maria Santa3     |1975-10-10 |250    |2025-09-26|\n",
      "|2    |ABCD1234EF567890 |Z0l0 Rossi       |1980-13-01 |1600   |2025-09-26|\n",
      "|2    |LMNOPQ12RST345678|Giulia1 Bianchi  |1990-02-30 |2850   |2025-09-26|\n",
      "|2    |QRSTUV98XYZ123456|Alessandro Verdi |1995-11-31 |abc    |2025-09-26|\n",
      "|2    |WXYZ1234ABCD56789|Federica Rossi   |1992-00-12 |0      |2025-09-26|\n",
      "+-----+-----------------+-----------------+-----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Tabella1 OK ===\")\n",
    "tabella1_ok.show()\n",
    "\n",
    "print(\"=== Tabella1 Scarti ===\")\n",
    "tabella1_scarti.show()\n",
    "\n",
    "print(\"=== Tabella1 OK ===\")\n",
    "tabella2_ok.show()\n",
    "\n",
    "print(\"=== Tabella1 Scarti ===\")\n",
    "tabella2_scarti.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3483e18-8fe8-41b8-a270-0e57e3df2284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+--------------+-------+\n",
      "|IDRUN|              CF|          NOME|SALARIO|\n",
      "+-----+----------------+--------------+-------+\n",
      "|    1|BNCLGU90B22F205X|  Luca Bianchi| 3200.0|\n",
      "|    1|NGRMRT75D10E345W|  Martina Neri| 3100.0|\n",
      "|    1|PPLGRD91D05E345Q|Paolo Pugliese| 3400.0|\n",
      "|    1|NGRMRT75D10E345W|   Maria Santa| 3100.0|\n",
      "+-----+----------------+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tab1 = tabella1_ok.GivemeDataFrame()\n",
    "df_filtrato = tab1.select(\"IDRUN\",\"CF\", \"NOME\", \"SALARIO\").where(col(\"SALARIO\") > 3000)\n",
    "df_filtrato.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc2b11a-9a04-41f6-97ae-7dc3a84d9a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ok_totale = tabella1_ok.GivemeDataFrame().unionByName(tabella2_ok.GivemeDataFrame())\n",
    "df_scarti_totale = tabella1_scarti.GivemeDataFrame().unionByName(tabella2_scarti.GivemeDataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56797e8d-8823-4f5e-a1ba-1b8badd3891e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+-----------------+----------+-------+----------+\n",
      "|IDRUN|              CF|             NOME|        DN|SALARIO|      DINS|\n",
      "+-----+----------------+-----------------+----------+-------+----------+\n",
      "|    1|RSSMRA85M01H501Z|      Mario Rossi|1985-03-15| 2500.0|2025-09-26|\n",
      "|    1|BNCLGU90B22F205X|     Luca Bianchi|1990-07-22| 3200.0|2025-09-26|\n",
      "|    1|NGRMRT75D10E345W|     Martina Neri|1975-12-10| 3100.0|2025-09-26|\n",
      "|    1|FBLGPP88A12H501V|    Filippo Baldi|1988-01-12| 2700.0|2025-09-26|\n",
      "|    1|GRLFRN83B25H501S|    Giorgia Ferri|1983-02-25| 2900.0|2025-09-26|\n",
      "|    1|PPLGRD91D05E345Q|   Paolo Pugliese|1991-05-05| 3400.0|2025-09-26|\n",
      "|    1|NGRMRT75D10E345W|      Maria Santa|1975-10-10| 3100.0|2025-09-26|\n",
      "|    2|TRNDMN85B12C123X|    Andrea Trenti|1985-02-12| 2600.0|2025-09-26|\n",
      "|    2|PVRGLU90F22D456Y|   Laura Pavarini|1990-07-22| 3300.0|2025-09-26|\n",
      "|    2|MRCDSP75E10F234W|Simona Mercadante|1975-11-10| 3150.0|2025-09-26|\n",
      "|    2|FNCGLP88G15H501V|Filippo Fanciulli|1988-01-15| 2750.0|2025-09-26|\n",
      "|    2|GRLFRN83B25H501S|    Giorgia Ferri|1983-02-25| 2950.0|2025-09-26|\n",
      "|    2|PPLGRD91D05E345Q|   Paolo Pugliese|1991-05-05| 3450.0|2025-09-26|\n",
      "|    2|NGRMRT75D10E345W| Martina Polgatti|1975-10-10| 3100.0|2025-09-26|\n",
      "|    2|NGRMRT75D10E345W|      Maria Santa|1975-10-10| 3100.0|2025-09-26|\n",
      "+-----+----------------+-----------------+----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ok_totale.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62e6cf-91bf-4eae-a01c-f9cad5065c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
