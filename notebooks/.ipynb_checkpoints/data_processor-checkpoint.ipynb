{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3eb90e21-586d-464c-85ef-950580dd5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType, IntegerType\n",
    "from pyspark.sql.functions import current_date, col, to_date, when, current_timestamp, monotonically_increasing_id, lit, upper, lower, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "954173b4-8fb2-44e3-84c8-6bce370a8baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestoreFlussoDipendenti:\n",
    "    def __init__(self, path_log_gestore, path_config_json):\n",
    "        self.contatore_idrun = 0\n",
    "        self.path_log_gestore = path_log_gestore\n",
    "\n",
    "        # Logger\n",
    "        self.logger = logging.getLogger(\"gestore\")\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        file_handler = logging.FileHandler(self.path_log_gestore, mode='w')\n",
    "        formatter = logging.Formatter(\"%(message)s\")\n",
    "        file_handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(file_handler)\n",
    "\n",
    "        # Carico la configurazione JSON\n",
    "        with open(path_config_json, 'r') as f:\n",
    "            self.config_all = json.load(f)\n",
    "\n",
    "    def genera_idrun(self):\n",
    "        self.contatore_idrun += 1\n",
    "        return self.contatore_idrun\n",
    "\n",
    "    def processa_flusso(self, path_csv, path_log, flusso_corrente):\n",
    "        idrun = self.genera_idrun()\n",
    "\n",
    "        # Creo oggetto TabellaDipendenti\n",
    "        tabella = TabellaDipendenti(path_csv, path_log, idrun)\n",
    "\n",
    "        # Recupero il dataframe grezzo\n",
    "        df_grezzo = tabella.givemedataframe()\n",
    "\n",
    "        # Recupero la configurazione per il flusso corrente\n",
    "        if flusso_corrente not in self.config_all:\n",
    "            raise ValueError(f\"Flusso {flusso_corrente} non trovato nella configurazione JSON\")\n",
    "\n",
    "        config_corrente = self.config_all[flusso_corrente][\"columns\"]\n",
    "\n",
    "        # Applico pulizia automatica\n",
    "        tab_ok, tab_scarti = tabella.pulisci(config_corrente)\n",
    "\n",
    "        # Log\n",
    "        self.logger.info(\n",
    "            f\"IDRUN={idrun} - Tabelle create: df_grezzo={df_grezzo.count()}, \"\n",
    "            f\"tab_ok={tab_ok.df.count()}, tab_scarti={tab_scarti.df.count()}, Data={datetime.now()}\"\n",
    "        )\n",
    "\n",
    "        return df_grezzo, tab_ok, tab_scarti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8fb683c2-6b82-4de2-b3b9-3aa5b914a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operazioni:\n",
    "    \n",
    "    '''Static method = Significa che puoi chiamare il metodo senza creare un oggetto della classe!'''\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_number(df, col_name):\n",
    "        return df.withColumn(\n",
    "            f\"valid_{col_name}_number\",\n",
    "            when(col(col_name).rlike(\"^[0-9]+(\\\\.[0-9]+)?$\"), True).otherwise(False)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def positive_number(df, col_name):\n",
    "        return df.withColumn(\n",
    "            f\"valid_{col_name}\",\n",
    "            when(col(col_name).rlike(\"^[0-9]+(\\\\.[0-9]+)?$\") & (col(col_name).cast(\"double\") >= 0), True).otherwise(False)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def string_no_numbers(df, col_name):\n",
    "        return df.withColumn(\n",
    "            f\"valid_{col_name}\",\n",
    "            when(col(col_name).isNotNull() & (~col(col_name).rlike(\"[0-9]\")), True).otherwise(False)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def date_format_regex(df, col_name):\n",
    "        # Estrazione anno, mese, giorno come interi\n",
    "        anno = col(col_name).substr(1,4).cast(\"int\")\n",
    "        mese = col(col_name).substr(6,2).cast(\"int\")\n",
    "        giorno = col(col_name).substr(9,2).cast(\"int\")\n",
    "\n",
    "        # Funzione di validazione mese/giorno\n",
    "        valid_day = (\n",
    "            ((mese.isin([1,3,5,7,8,10,12])) & (giorno.between(1,31))) |  # mesi con 31 giorni\n",
    "            ((mese.isin([4,6,9,11])) & (giorno.between(1,30))) |          # mesi con 30 giorni\n",
    "            ((mese == 2) & (giorno.between(1,28))) |                      # febbraio normale\n",
    "            ((mese == 2) & (giorno == 29) & ((anno % 4 == 0) & ((anno % 100 != 0) | (anno % 400 == 0))))  # bisestile\n",
    "            )\n",
    "\n",
    "        return df.withColumn(\n",
    "            f\"valid_{col_name}\",\n",
    "            when(\n",
    "                (col(col_name).rlike(r'^\\d{4}-\\d{2}-\\d{2}$')) & valid_day,\n",
    "                True\n",
    "            ).otherwise(False)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def email(df, col_name):\n",
    "        pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
    "        return df.withColumn(\n",
    "            f\"valid_{col_name}\",\n",
    "            when(col(col_name).rlike(pattern), True).otherwise(False)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def string_length(df, col_name, length_val):\n",
    "        \"\"\"\n",
    "        Controlla che la colonna col_name abbia esattamente length_val caratteri.\n",
    "        \"\"\"\n",
    "        return df.withColumn(\n",
    "            f\"valid_{col_name}\",\n",
    "            when(\n",
    "                col(col_name).isNotNull() & (length(col(col_name)) == length_val),\n",
    "                True\n",
    "            ).otherwise(False)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ab4bbec5-1e1b-40f0-961a-78bc2b16604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabellaDipendenti:\n",
    "\n",
    "    # Costruttore\n",
    "    def __init__(self, path_csv, path_log, idrun):\n",
    "            self.spark = (\n",
    "                SparkSession.builder\n",
    "                .master(\"local[1]\")\n",
    "                .appName(\"FlussoGenerico\")\n",
    "                .getOrCreate()\n",
    "            )\n",
    "\n",
    "            self.idrun = idrun\n",
    "            self.path_log = path_log\n",
    "\n",
    "            # Lettura generica: header dalla prima riga, tutto come stringa\n",
    "            self.df = (\n",
    "                self.spark.read\n",
    "                .option(\"header\", True)   # la prima riga diventa header\n",
    "                .option(\"inferSchema\", False)  # tutto stringa, conversione dopo\n",
    "                .csv(path_csv)\n",
    "            )\n",
    "\n",
    "            # Aggiungo data inserimento\n",
    "            self.df = self.df.withColumn(\"DINS\", current_timestamp())\n",
    "\n",
    "            # Logging configurazione\n",
    "            logging.basicConfig(\n",
    "                filename=self.path_log,\n",
    "                filemode='w',\n",
    "                level=logging.INFO,\n",
    "                format=\"%(message)s\",\n",
    "                force=True\n",
    "            )\n",
    "\n",
    "            # Log iniziale\n",
    "            logging.info(\n",
    "                \"IDRUN=%s, Operazione=Costruttore, Stato=OK, File=%s, Data=%s\",\n",
    "                idrun, path_csv, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            )\n",
    "\n",
    "    # Funzione che mostra la \"tabella\". \n",
    "    def show(self):\n",
    "        if self.df:\n",
    "            self.df.show(truncate=False)\n",
    "            logging.info(\"IDRUN=%s, Operazione=Show, Stato=OK, Righe=%s, Data=%s\", \n",
    "             self.idrun, self.df.count(), datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        else:\n",
    "            print(\"DataFrame vuoto.\")\n",
    "            logging.info(\"IDRUN=%s, Operazione=Show, Stato=VUOTO, Data=%s\", \n",
    "             self.idrun, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    def printSchema(self):\n",
    "        if self.df:\n",
    "            self.df.printSchema()\n",
    "            logging.info(\n",
    "                \"IDRUN=%s, Operazione=printSchema, Stato=OK, Data=%s\",\n",
    "                self.idrun,\n",
    "                datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            )\n",
    "        else:\n",
    "            print(\"DataFrame vuoto.\")\n",
    "            logging.info(\n",
    "                \"IDRUN=%s, Operazione=printSchema, Stato=VUOTO, Data=%s\",\n",
    "                self.idrun,\n",
    "                datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            )\n",
    "            \n",
    "    def givemedataframe(self):\n",
    "        logging.info(\"IDRUN=%s, Operazione=givemedataframe, Stato=OK, Data=%s\", \n",
    "             self.idrun, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        return self.df\n",
    "    \n",
    "    def pulisci(self, config):\n",
    "        df = self.df\n",
    "\n",
    "        # 1. Applica le regole del JSON\n",
    "        for col_name, rule in config.items():\n",
    "            if col_name not in df.columns:\n",
    "                logging.warning(f\"Colonna {col_name} non trovata nel DataFrame, salto la validazione\")\n",
    "                continue\n",
    "\n",
    "            rule_type = rule[\"type\"]\n",
    "\n",
    "            if rule_type == \"positive_number\":\n",
    "                df = Operazioni.positive_number(df, col_name)\n",
    "            elif rule_type == \"string_no_numbers\":\n",
    "                df = Operazioni.string_no_numbers(df, col_name)\n",
    "            elif rule_type == \"date\":\n",
    "                df = Operazioni.date_format_regex(df, col_name)\n",
    "            elif rule_type == \"email\":\n",
    "                df = Operazioni.email(df, col_name)\n",
    "            elif rule_type == \"string_length\":\n",
    "                length_val = rule.get(\"length\", 0)\n",
    "                df = df.withColumn(\n",
    "                    f\"valid_{col_name}\",\n",
    "                    when(col(col_name).isNotNull() & (length(col(col_name)) == length_val), True).otherwise(False)\n",
    "                )\n",
    "\n",
    "            # Logging dinamico\n",
    "            count_valid = df.filter(col(f\"valid_{col_name}\") == True).count()\n",
    "            logging.info(\n",
    "                \"IDRUN=%s, Operazione=pulisci, Filtro=%s, Righe OK=%s, Data=%s\",\n",
    "                self.idrun, col_name, count_valid, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            )\n",
    "\n",
    "        # 2. Colonna finale valid = AND di tutte le valid_<col>\n",
    "        valid_cols = [f\"valid_{c}\" for c in config.keys() if f\"valid_{c}\" in df.columns]\n",
    "        if valid_cols:\n",
    "            df = df.withColumn(\"valid\", reduce(lambda a, b: a & b, [col(c) for c in valid_cols]))\n",
    "        else:\n",
    "            df = df.withColumn(\"valid\", lit(True))  # se non ci sono colonne, tutto OK\n",
    "\n",
    "        # 3. DF OK → righe valide con conversioni\n",
    "        df_ok = df.filter(col(\"valid\") == True)\n",
    "        for col_name, rule in config.items():\n",
    "            if col_name not in df_ok.columns:\n",
    "                continue\n",
    "            if rule[\"type\"] == \"positive_number\":\n",
    "                df_ok = df_ok.withColumn(col_name, col(col_name).cast(\"double\"))\n",
    "            elif rule[\"type\"] == \"date\":\n",
    "                df_ok = df_ok.withColumn(col_name, to_date(col(col_name), \"yyyy-MM-dd\"))\n",
    "\n",
    "        df_ok = df_ok.withColumn(\"IDRUN\", lit(self.idrun)).withColumn(\"DINS\", current_timestamp())\n",
    "        df_ok = df_ok.drop(*valid_cols, \"valid\").select(\"IDRUN\", *[c for c in self.df.columns if c != \"DINS\"], \"DINS\")\n",
    "\n",
    "        # 4. DF SCARTI → righe non valide, tutte stringhe\n",
    "        df_scarti = df.filter(col(\"valid\") == False)\n",
    "        df_scarti = df_scarti.withColumn(\"IDRUN\", lit(self.idrun)).withColumn(\"DINS\", current_timestamp())\n",
    "        df_scarti = df_scarti.drop(*valid_cols, \"valid\").select(\"IDRUN\", *[c for c in self.df.columns if c != \"DINS\"], \"DINS\")\n",
    "\n",
    "        # 5. Creo nuovi oggetti TabellaDipendenti\n",
    "        tabella_ok = TabellaDipendenti.__new__(TabellaDipendenti)\n",
    "        tabella_ok.spark = self.spark\n",
    "        tabella_ok.df = df_ok\n",
    "        tabella_ok.idrun = self.idrun\n",
    "\n",
    "        tabella_scarti = TabellaDipendenti.__new__(TabellaDipendenti)\n",
    "        tabella_scarti.spark = self.spark\n",
    "        tabella_scarti.df = df_scarti\n",
    "        tabella_scarti.idrun = self.idrun\n",
    "\n",
    "        return tabella_ok, tabella_scarti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c692d6bb-bc7e-46a4-955a-6871147c7b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config_json = \"../data/config_flussi.json\"\n",
    "path_log_gestore = \"../logs/tlog_gestore.log\"\n",
    "gestore = GestoreFlussoDipendenti(path_log_gestore, path_config_json)\n",
    "\n",
    "path_log = \"../logs/tlog.log\"\n",
    "path_flusso = \"../data/unitacr.csv\"\n",
    "chiave_json = \"unitacr\"\n",
    "df_grezzo, tab_ok, tab_scarti = gestore.processa_flusso(path_flusso, path_log, chiave_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "46af57eb-6a98-4689-823f-e17b4f76eec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+--------------------------+\n",
      "|IDRUN|COD_UNT|COD_ACR|DINS                      |\n",
      "+-----+-------+-------+--------------------------+\n",
      "|1    |15311.0|CDI    |2025-09-27 17:03:08.547233|\n",
      "|1    |15312.0|CDD    |2025-09-27 17:03:08.547233|\n",
      "|1    |15357.0|CSS    |2025-09-27 17:03:08.547233|\n",
      "|1    |15358.0|CSS    |2025-09-27 17:03:08.547233|\n",
      "|1    |15360.0|CDI    |2025-09-27 17:03:08.547233|\n",
      "|1    |15370.0|CDI    |2025-09-27 17:03:08.547233|\n",
      "|1    |15445.0|RSA    |2025-09-27 17:03:08.547233|\n",
      "|1    |15660.0|STD    |2025-09-27 17:03:08.547233|\n",
      "|1    |15820.0|PSD    |2025-09-27 17:03:08.547233|\n",
      "|1    |20310.0|RSA    |2025-09-27 17:03:08.547233|\n",
      "|1    |20320.0|RSD    |2025-09-27 17:03:08.547233|\n",
      "|1    |30340.0|RSD    |2025-09-27 17:03:08.547233|\n",
      "|1    |40210.0|RSA    |2025-09-27 17:03:08.547233|\n",
      "|1    |40230.0|RGG    |2025-09-27 17:03:08.547233|\n",
      "|1    |40420.0|STR    |2025-09-27 17:03:08.547233|\n",
      "|1    |60320.0|IDR    |2025-09-27 17:03:08.547233|\n",
      "|1    |60321.0|RDC    |2025-09-27 17:03:08.547233|\n",
      "|1    |60322.0|CDP    |2025-09-27 17:03:08.547233|\n",
      "|1    |60324.0|CEM    |2025-09-27 17:03:08.547233|\n",
      "|1    |60330.0|HOS    |2025-09-27 17:03:08.547233|\n",
      "+-----+-------+-------+--------------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "tab_ok.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "836ac6e1-1c21-4dc2-b356-a93de7b73265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+--------------------+\n",
      "|IDRUN|COD_UNT|COD_ACR|                DINS|\n",
      "+-----+-------+-------+--------------------+\n",
      "|    1|15445.0|    RSA|2025-09-27 17:03:...|\n",
      "|    1|20310.0|    RSA|2025-09-27 17:03:...|\n",
      "|    1|40210.0|    RSA|2025-09-27 17:03:...|\n",
      "+-----+-------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unitacr = tab_ok.givemedataframe()\n",
    "unitacr.select(\"*\").where(col(\"COD_ACR\") == \"RSA\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "abc8a522-c2f6-4e5e-9dff-861f6accb1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IDRUN: integer (nullable = false)\n",
      " |-- COD_UNT: double (nullable = true)\n",
      " |-- COD_ACR: string (nullable = true)\n",
      " |-- DINS: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tab_ok.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f7f22a94-5642-4bd4-a66e-4a9487dc85ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SALARIO {'type': 'positive_number'}\n",
      "DN {'type': 'date', 'format': 'yyyy-MM-dd'}\n",
      "NOME {'type': 'string_no_numbers'}\n",
      "CF {'type': 'string_length', 'length': 16}\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "for col_name, rule in config_corrente.items():\n",
    "    print(col_name, rule)\n",
    "    \n",
    "print(rule.get(\"length\", 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
